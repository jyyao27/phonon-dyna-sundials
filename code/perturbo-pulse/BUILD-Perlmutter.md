# Build and Run Perturbo on Perlmutter with MPI, OpenMP and OpenACC

These directions show how to set up the Perturbo program to run on the
NERSC Perlmutter compute cluster, using multiple compute nodes with MPI,
multiple CPUs with OpenMP, and GPU acceleration with OpenACC.

The NVIDIA compilers are pretty fussy, but the fussiness seems to be
exacerbated by using OpenBLAS.  The NVIDIA HPC Toolkit includes GPU-aware
libraries like cublas, and using the NVIDIA-provided libraries seems to
produce a much more stable program.

> **NOTE:**  Currently, NERSC is not keeping up with the latest NVIDIA HPC
> Toolkit versions on Perlmutter.  Currently only the 22.7 version works
> with all the other tools available on Perlmutter.

## Step 1:  Set up Perlmutter environment.

Make sure to switch to the NVIDIA HPC Toolkit version 22.7.  (Earlier versions
have OpenMP bugs that break Perturbo, and later versions are not yet fully
supported on Perlmutter.)

```sh
module load nvhpc/22.7
module load cray-hdf5
```

NERSC's recommendation on Perlmutter is that their compiler wrappers ought
to be used to set the proper arguments to run with MPI.  The wrappers are
`cc` for the C compiler, `CC` for the C++ compiler, and `ftn` for the Fortran
compiler.  Also, note that the QuantumEspresso 7.2 build process uses `cpp`,
the C preprocessor.

## Step 2:  Download, configure and build QuantumEspresso 7.2.

QuantumEspresso (QE) has a pretty straightforward build process, but there
are a few wrinkles with the NVIDIA compiler.

First, clone the QE repository and switch to version 7.2.

```sh
# Go to the home directory to download QuantumEspresso
cd ~
git clone https://gitlab.com/QEF/q-e.git

# Switch to the version of QuantumEspresso tagged as 7.2
cd q-e
git checkout qe-7.2
```

You can configure QE to build on Perlmutter using the NERSC-specified compiler
wrappers.  **Note:**  These configuration options will cause QE to use MPI,
OpenMP and OpenACC.  These technologies can be disabled with other arguments;
example commands are given below.

> **NOTE:**  Among other tweaks required to this command, `sassy` also
> requires `--with-cuda-cc=75` instead of `--with-cuda-cc=80`.  QE doesn't
> support multiple values such as "75,80"; it requires that only one value be
> specified.

```sh
# Configure QuantumEspresso using NERSC's compiler wrappers, and additional
# compiler switches required by Perturbo.
# Note that this is one single command wrapped across multiple lines.
FC=ftn F90=ftn MPIF90=ftn CC=cc \
    FFLAGS="-O3 -Mlarge_arrays -mcmodel=medium" CFLAGS="-O3 -mcmodel=medium" \
    LDFLAGS="-L/opt/nvidia/hpc_sdk/Linux_x86_64/22.7/math_libs/lib64" \
    BLAS_LIBS="-lblas" LAPACK_LIBS="-llapack" \
    ./configure --enable-parallel=yes --enable-openmp=yes \
    --with-cuda=/opt/nvidia/hpc_sdk/Linux_x86_64/22.7/cuda \
    --with-cuda-cc=80 --with-cuda-runtime=11.7
```

If OpenACC is enabled (which it is by default), then there's currently a bad
interaction between the NVIDIA Fortran compiler and the QE7.2 build process
which now includes a preprocessor:  The preprocessor doesn't have the
`_OPENACC` symbol defined, which causes some of the preprocessed code to be
mangled in ways that confuse the compiler.

I have tried many solutions, and the simplest thing to do is just edit the
`make.inc` file generated by the above command to include the `_OPENACC`
symbol on the preprocessor arguments.  Open `make.inc` and find this section:

```make
# C preprocessor and preprocessing flags - for explicit preprocessing,
# if needed (see the compilation rules above)
# preprocessing flags must include DFLAGS and IFLAGS

CPP            = cpp 
CPPFLAGS       = -P -traditional -Uvector $(DFLAGS) $(IFLAGS)
```

Add `-D_OPENACC` to the definition of `CPPFLAGS`, so it looks like this:

```make
# C preprocessor and preprocessing flags - for explicit preprocessing,
# if needed (see the compilation rules above)
# preprocessing flags must include DFLAGS and IFLAGS

CPP            = cpp 
CPPFLAGS       = -P -traditional -Uvector -D_OPENACC $(DFLAGS) $(IFLAGS)
```

> TODO:  You may also want to change the definition of `F90` from `ftn` to
> `nvfortran` until we make the Perturbo build script more intelligent.

Finally, build all of QuantumEspresso 7.2.

```sh
# Build all of QE
make clean
make all -j
```

## Step 3:  Build Perturbo.

The version of Perturbo in the `perturbo-sase` project has a `make.sys` file
that should work as-is on Perlmutter.

```sh
# Start in the ~/q-e directory!
git clone git@github.com:donnie-caltech/perturbo-sase.git
cd perturbo-sase
# TODO:  Check out the desired branch of this repository here.

make clean
make
```

## Step 4:  Running Perturbo with `perturbopy` test suite.

Here are some instructions for running the
[`perturbopy` test suite](https://perturbopy.readthedocs.io/en/latest/index.html)
against the Perturbo build based on the setup above.

I don't know the best environment to use for Python on Perlmutter, but this
setup seems to work.

Check out the `perturbopy` project and set it up with a local Python
virtual environment.  I do this from my home directory:

```sh
# Go back to the user's top-level directory.
cd ~

# Set up Python and install virtualenv project.
module load python/3.11
pip install virtualenv
export PATH=$PATH:~/.local/bin

# Check out perturbopy sources, and set up a virtual environment
# inside the project.
git clone https://github.com/perturbo-code/perturbopy.git
cd perturbopy
virtualenv venv
source venv/bin/activate

# Initialze the perturbopy project into the virtual environment.
pip install .

# Point perturbopy at the Perturbo build produced from the earlier steps.
export PERTURBO_RUN=~/q-e/perturbo-sase/bin/perturbo.x
```

Now it should be possible to run the `perturbopy` tests following the
usual instructions.

```sh
cd ~/perturbopy/tests
pytest
```

# Configuring the Environment on Login

Making sure to run through all the environment configuration steps each time
time can be a big pain.  Therefore, it is convenient to create a `.bashrc`
file in your home directory that performs these steps every time.  In a file
`~/.bashrc`, put the following contents:

```sh
module load nvhpc/22.7
module load cray-hdf5
module load python/3.11

export PATH=$PATH:~/.local/bin
export NVHPC_CUDA_HOME=$CUDA_HOME
export PERTURBO_RUN=~/q-e/perturbo-sase/bin/perturbo.x
```

This way, the environment configuration described in previous sections will
occur automatically at login.

# Disabling MPI, OpenMP, and/or OpenACC

Sometimes it is desirable to disable various technologies used by
QuantumEspresso and Perturbo.  This section shows how to disable MPI, OpenMP
and/or OpenACC, as desired.

Note that if a feature is to be disabled, it generally must be disabled both
in QuantumEspresso and in Perturbo.  This will often require multiple steps,
since these projects have separate and independent build processes.  It is
also a very good idea to do a clean rebuild of the software when changing
these configurations.

Generally, changes to QuantumEspresso configuration are made by issuing a new
`configure` command with different parameters, then doing a clean rebuild of
QE.  Changes to Perturbo configuration are made by updating the
`perturbo-sase/make.sys` file, then doing a clean rebuild of Perturbo.

## Disabling MPI Only

To disable MPI only, QuantumEspresso must be rebuilt with this configuration.
Note that `--enable-parallel=no` instead of the configuration given
previously.  The full command is given here for convenience.

```sh
# Configure QuantumEspresso with MPI disabled.
# Note that this is one single command wrapped across multiple lines.
FC=ftn F90=ftn MPIF90=ftn CC=cc FCFLAGS="-Mlarge_arrays -mcmodel=medium" \
    LIBS="-lblas -llapack" CFLAGS="-mcmodel=medium" \
    LDFLAGS="-L/opt/nvidia/hpc_sdk/Linux_x86_64/22.7/math_libs/lib64" \
    ./configure --enable-parallel=no --enable-openmp=yes \
    --with-cuda=/opt/nvidia/hpc_sdk/Linux_x86_64/22.7/cuda \
    --with-cuda-cc=80 --with-cuda-runtime=11.7

# Build all of QE
make clean
make all -j
```

No changes are required to `perturbo-sase/make.sys` to disable MPI; it draws
from QuantumEspresso's configuration.  **However, a clean rebuild of Perturbo
is still necessary.**

## Disabling OpenMP Only

To disable OpenMP only, QuantumEspresso must be rebuilt with this
configuration.  Note that `--enable-openmp=no` instead of the configuration
given previously.  The full command is given here for convenience.

```sh
# Configure QuantumEspresso with OpenMP disabled.
# Note that this is one single command wrapped across multiple lines.
FC=ftn F90=ftn MPIF90=ftn CC=cc FCFLAGS="-Mlarge_arrays -mcmodel=medium" \
    LIBS="-lblas -llapack" CFLAGS="-mcmodel=medium" \
    LDFLAGS="-L/opt/nvidia/hpc_sdk/Linux_x86_64/22.7/math_libs/lib64" \
    ./configure --enable-parallel=yes --enable-openmp=no \
    --with-cuda=/opt/nvidia/hpc_sdk/Linux_x86_64/22.7/cuda \
    --with-cuda-cc=80 --with-cuda-runtime=11.7

# Build all of QE
make clean
make all -j
```

Disabling OpenMP in Perturbo also requires changes to
`perturbo-sase/make.sys`, since NVIDIA's compilers try to use thread-level
parallelism quite aggressively.  Find the following section of `make.sys` and
configure it to look like this:

```makefile
# OpenMP - target CPUs
# --------------------
# To enable OpenMP:
# FFLAGS += -mp=multicore
# LDFLAGS += -mp=multicore

# To disable OpenMP:
FFLAGS += -nomp
LDFLAGS += -nomp
```

Then, do a clean rebuild of Perturbo.

## Disabling OpenACC Only

QuantumEspresso doesn't expose a particularly good mechanism for disabling
OpenACC across the board.  It has a `--enable-openacc=no` switch that can be
passed to QE's `configure` script, but this switch doesn't affect the
MPI-compiler arguments used by QE, and this makes it largely useless for
disabling OpenACC.

Fortunately, this isn't a big deal.  OpenACC can be disabled in Perturbo
only, and this is adequate for achieving the goal.

As with OpenMP, disabling OpenACC in Perturbo requires changes to
`perturbo-sase/make.sys`, since NVIDIA's compilers try to use GPU acceleration
rather aggressively.  Find the following section of `make.sys` and configure
it to look like this:

```makefile
# OpenACC - target NVIDIA GPUs (sassy/perlmutter)
# -----------------------------------------------
# [explanatory comments snipped out]

# To enable OpenACC:
# FFLAGS += -acc=gpu -Minfo=accel -gpu=cc80,nomanaged,deepcopy,zeroinit
# LDFLAGS += -acc=gpu -gpu=cc80,nomanaged,deepcopy,zeroinit

# To disable OpenACC:
#  - NOTE:  Must NOT add -noacc to LDFLAGS if QE uses OpenACC
FFLAGS += -noacc
# LDFLAGS += -noacc
```

Then, do a clean rebuild of Perturbo.

As the `make.sys` comments explain, `-noacc` can only be specified during the
compile phase, since portions of QuantumEspresso will also use OpenACC
regardless of how you configure it.  Therefore, the `LDFLAGS` cannot be set
to specify `-noacc` without generating link-time warnings.

## Disabling Multiple Technologies

Of course, multiple of the above technologies can be disabled as well.
Simply make sure to combine the configuration instructions of the sections
appropriately, and make sure to do a clean rebuild of all affected parts of
the program.

> **NOTE:**  If QuantumEspresso's configuration needs to be updated, then
> reconfigure and rebuild it first, before reconfiguring and rebuilding
> Perturbo.

# Running Perturbo with MPI/SLURM on Perlmutter GPU Nodes

Assuming Perturbo has been built with MPI enabled, it's straightforward to
run it on a group of compute nodes on Perlmutter.  This section gives some
example instructions that seem to work so far.

Since running Perturbo on Perlmutter is still very new, we only have setup
instructions for interactive nodes.  More instructions will be forthcoming
as we figure out how to do more with Perturbo on Perlmutter.

> **IMPORTANT NOTE:**  Perturbo currently only knows how to use ONE GPU per
> process for accelerating its computations.  If multiple GPUs are required
> for a given calculation then Perturbo must be run in an MPI cluster where
> each task (MPI process) is assigned a single dedicated GPU.

## Interactive Execution

First, use `salloc` to allocate as many compute nodes as are desired.  Recall
the compute capabilities of Perlmutter nodes:

*   GPU compute nodes have one AMD EPYC 7763 CPU with 64 physical cores (128
    logical cores), and 4x NVIDIA A100 Tensor Core GPUs (compute capability =
    cc80).  The account specified on the `salloc` command must end in `_g` to
    work properly.

*   CPU nodes have two AMD EPYC 7763 CPUs with a total of 128 physical cores
    (256 logical cores), and no GPUs.

CPU-only compute nodes can be allocated if OpenACC isn't going to be used in
the computation.

In this example, we are running on two GPU compute nodes, each of which has 4
GPUs.  Thus, we want to run with 8 MPI tasks (MPI processes), and one GPU
assigned to each of these tasks.  We also want to divide the CPU cores evenly
amongst all these MPI tasks.  (When we speak of cores below, we mean logical
cores, not physical cores.)

```sh
# Allocate 2 GPU compute nodes, for an interactive session lasting 30 minutes.
# Of course, replace <group_number> with the group to be billed for the time.
salloc -N 2 -C gpu -A m<group_number>_g -q interactive -t 30:00
```

Once `salloc` returns with a compute-node allocation, Perturbo can be run
with relative ease.

```sh
# This occurs on the compute node allocated with salloc.

# This is a single command wrapped across multiple lines.
export OMP_NUM_THREADS=32
srun -A m<group_number>_g -N 2 -n 8 --cpus-per-task=32 --cpu_bind=cores \
    --gpus-per-task=1 --gpu-bind=closest \
    ../bin/perturbo.x -npools 8 -i pert.in
```

Here is an explanation of the various arguments in these commands.  Note that
several of these values are related to each other.

*   `-N 2` means Perturbo is running on two separate compute nodes.

*   `-n 8` means run 8 tasks (MPI processes) total.  For GPU nodes, this will
    typically be 4x the number of compute nodes, since each MPI task should
    only have one GPU assigned to it, and Perlmutter GPU nodes have 4 GPUs
    each.

*   `--cpus-per-task=32` means each task (MPI process) is assigned 32
    (logical) CPU cores.  This value should match the `OMP_NUM_THREADS` value.

*   `--cpu_bind=cores` tells SLURM that "CPUs" means "CPU cores" for us.

*   `--gpus-per-task=1` tells SLURM that each task (MPI process) should have
    a single GPU assigned to it.  This should always be set to 1 since
    Perturbo doesn't know how to make use of multiple GPUs in a single
    process.

*   `--gpu-bind=closest` tells SLURM to assign each task a GPU that is
    close to it on the system.

Important constraints to consider are:

*   The number of tasks (`-n`) multiplied by the number of CPUs (cores) per
    task (`--cpus-per-task`) should not exceed the total number of CPU cores
    in the cluster.  Above, we have 8 tasks and 32 (logical) cores per task;
    this is a total of 256 (logical) cores, which doesn't exceed the total
    number of logical cores on two Perlmutter GPU nodes.

*   The `OMP_NUM_THREADS` value should match the `--cpus-per-task` value.

*   The `-npools` value specified to Perturbo must match the total number of
    MPI tasks (argument to `-n`).

### Running `perturbopy` Tests in Interactive Session

To run the `perturbopy` tests in an interactive session, several environment
variables must be set.  The `PERTURBO_RUN` argument is the most important;
note that it is identical to the `srun` command in the previous section, but
the entire command must be double-quoted, and the path to `perturbo.x` must
be an absolute path.

```sh
# This is same as before.
export OMP_NUM_THREADS=32

# This environment variable tells perturbopy how to run Perturbo.
# Note that an absolute path must be specified!
export PERTURBO_RUN="srun -A m<group_number>_g -N 2 -n 8 --cpus-per-task=32 --cpu_bind=cores --gpus-per-task=1 --gpu-bind=closest /<absolute_path_to_perturbo>/perturbo.x -npools 8"
```

> **NOTE:**  If you are using `virtualenv` to manage your Python environment,
> don't forget to activate the environment before attempting to run tests!


